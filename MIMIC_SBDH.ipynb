{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knguyen1228/deep_learning_reproduction/blob/main/MIMIC_SBDH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l2YdyJXDUWD"
      },
      "source": [
        "# Task\n",
        "Create data loader and load it into the pandas table\n",
        "\n",
        "Here is all the data you need:\n",
        "\"MIMIC-SBDH-keywords.csv\"\n",
        "\"MIMIC-SBDH.csv\"\n",
        "\"NOTEEVENTS.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KLX31ab7KsE",
        "outputId": "db386a73-324b-4f1e-e580-a5d18ecd6df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.12\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "print(platform.python_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDRXSeIjWYOZ",
        "outputId": "0adbc247-a377-4959-c9e5-998758a39504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/hussameldinanwer/noteevents-mimic-iii?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.10G/1.10G [00:30<00:00, 38.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/hussameldinanwer/noteevents-mimic-iii/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"hussameldinanwer/noteevents-mimic-iii\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "wu54ZmRAnbVu",
        "outputId": "d981a575-67c8-4151-a106-2619f4cc652e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-7b5689b0c210>:10: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  df_noteevents = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming download from 4007718005 bytes (-2826803598 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/hussameldinanwer/noteevents-mimic-iii?dataset_version_number=1&file_name=NOTEEVENTS.csv (4007718005/1180914407) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3.73GB [00:00, 331kB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DataCorruptionError",
          "evalue": "The X-Goog-Hash header indicated a MD5 checksum of:\n\n  UYCsLk4nXo2NOWLWjoHUYA==\n\nbut the actual MD5 checksum of the downloaded contents was:\n\n  1HXK51ChkLPCxK+qrBpuqA==\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataCorruptionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b5689b0c210>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m df_noteevents = kagglehub.load_dataset(\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mKaggleDatasetAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPANDAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m\"hussameldinanwer/noteevents-mimic-iii\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;34m\"load_dataset is deprecated and will be removed in a future version.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             return kagglehub.pandas_datasets.load_pandas_dataset(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Now that everything has been validated, we can start downloading and processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         result = read_function(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/http_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Downloading a single file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mapi_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_auto_compressed_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# TODO(b/345800027) Implement parallel download when < 25 files in databundle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mactual_md5_hash\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_md5_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Delete the corrupted file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     raise DataCorruptionError(\n\u001b[0m\u001b[1;32m    224\u001b[0m                         \u001b[0m_CHECKSUM_MISMATCH_MSG_TEMPLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_md5_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_md5_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     )\n",
            "\u001b[0;31mDataCorruptionError\u001b[0m: The X-Goog-Hash header indicated a MD5 checksum of:\n\n  UYCsLk4nXo2NOWLWjoHUYA==\n\nbut the actual MD5 checksum of the downloaded contents was:\n\n  1HXK51ChkLPCxK+qrBpuqA==\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"NOTEEVENTS.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df_noteevents = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"hussameldinanwer/noteevents-mimic-iii\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6q8Mc8qpNvV"
      },
      "outputs": [],
      "source": [
        "display(df[df['ROW_ID'] == 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSABGQ0eqMEO"
      },
      "outputs": [],
      "source": [
        "display(df[df['ROW_ID'] == 5]['TEXT'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlTzLFHeAmP8"
      },
      "outputs": [],
      "source": [
        "text_value = df[df['ROW_ID'] == 5]['TEXT'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlW0UcS6Am19"
      },
      "outputs": [],
      "source": [
        "print(text_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUBiBsJDWW5p"
      },
      "outputs": [],
      "source": [
        "# prompt: extract Social history section without header for all rows\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def extract_social_history(text):\n",
        "    \"\"\"\n",
        "    Extracts the Social History section from a given text.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    pattern = r\"social history:\\s*(.*?)(?=\\n[a-z\\s]+:|\\Z)\"\n",
        "    social_history_match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "    if social_history_match:\n",
        "        return social_history_match.group(1).strip()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Apply the function to extract social history for each row\n",
        "df_noteevents['Social History'] = df_noteevents['TEXT'].apply(extract_social_history)\n",
        "\n",
        "# Display the extracted social history\n",
        "print(df_noteevents['Social History'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzBP0PHdYr9v"
      },
      "outputs": [],
      "source": [
        "# Extract Social History using regex\n",
        "df['Social_History'] = df_noteevents['TEXT'].str.extract(r'Social History:\\s*(.*)')\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df['Social_History'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPSCIhD7qrxs"
      },
      "outputs": [],
      "source": [
        "# Assuming you have the DataFrame 'df' and the column 'TEXT'\n",
        "text_value = df[df['ROW_ID'] == 5]['TEXT'].values[0]  # Get the text value\n",
        "substring = text_value[534:540]  # Extract the substring\n",
        "\n",
        "print(substring)  # Print the substring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7Bi78kcqyy6"
      },
      "outputs": [],
      "source": [
        "substring = text_value[543:547]  # Extract the substring\n",
        "print(substring)  # Print the substring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XaHwOpUiHVO"
      },
      "outputs": [],
      "source": [
        "distinct_row_ids = df['ROW_ID'].unique()\n",
        "\n",
        "print(distinct_row_ids.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tr-0xgjiR_x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GueiqDaPiQqJ"
      },
      "outputs": [],
      "source": [
        "display(df['ROW_ID'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSbQeMEj8Tny"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCt__lN1DUlw"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the three CSV files into pandas DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6CAvVxwDVYd"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to load the three CSV files into pandas DataFrames using the specified function and handle potential FileNotFoundError exceptions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcP8V6ffDVoL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(keywords_file, sbdh_file, notes_file):\n",
        "    \"\"\"Loads three CSV files into pandas DataFrames.\n",
        "\n",
        "    Args:\n",
        "        keywords_file: Path to \"MIMIC-SBDH-keywords.csv\".\n",
        "        sbdh_file: Path to \"MIMIC-SBDH.csv\".\n",
        "        notes_file: Path to \"NOTEEVENTS.csv\".\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing three DataFrames: (df_keywords, df_sbdh, df_notes).\n",
        "        Returns None for a DataFrame if the corresponding file is not found.\n",
        "    \"\"\"\n",
        "    df_keywords = None\n",
        "    df_sbdh = None\n",
        "    df_notes = None\n",
        "    try:\n",
        "      df_keywords = pd.read_csv(keywords_file)\n",
        "    except FileNotFoundError:\n",
        "      print(f\"Error: {keywords_file} not found.\")\n",
        "    try:\n",
        "      df_sbdh = pd.read_csv(sbdh_file)\n",
        "    except FileNotFoundError:\n",
        "      print(f\"Error: {sbdh_file} not found.\")\n",
        "    try:\n",
        "      df_notes = pd.read_csv(notes_file)\n",
        "    except FileNotFoundError:\n",
        "      print(f\"Error: {notes_file} not found.\")\n",
        "    return df_keywords, df_sbdh, df_notes\n",
        "\n",
        "df_keywords, df_sbdh, df_notes = load_data(\"/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH-keywords.csv\", \"/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv\", \"/content/drive/MyDrive/Colab_Notebooks/NOTEEVENTS.csv\")\n",
        "\n",
        "if df_keywords is not None:\n",
        "    display(df_keywords.head())\n",
        "if df_sbdh is not None:\n",
        "    display(df_sbdh.head())\n",
        "if df_notes is not None:\n",
        "    display(df_notes.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NawqxCuDjCks"
      },
      "outputs": [],
      "source": [
        "display(df_sbdh['row_id'].unique())\n",
        "print(df_sbdh['row_id'].unique().size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VDTYwPT05ft"
      },
      "outputs": [],
      "source": [
        "filtered_df = df_noteevents[df_noteevents['ROW_ID'].isin(df_sbdh['row_id'])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX6JJOlezNqa"
      },
      "outputs": [],
      "source": [
        "display(filtered_df[['ROW_ID', 'TEXT']]) # Use a list of column names to select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoSG_6jwaiGd"
      },
      "outputs": [],
      "source": [
        "filtered_df['Social_History'] = filtered_df['TEXT'].apply(extract_social_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USa9hUnwatM3"
      },
      "outputs": [],
      "source": [
        "display(filtered_df[['ROW_ID', 'Social_History']]) # Use a list of column names to select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2K0usdIbS6U"
      },
      "outputs": [],
      "source": [
        "# prompt: generate csv file of filtered_df with ROW_ID, TEXT, and Social_History\n",
        "\n",
        "# Assuming filtered_df is already created as in the previous code\n",
        "filtered_df[['ROW_ID', 'Social_History']].to_csv('filtered_social_history_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xv9_s9LbQjE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ7faoYG-32H"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade spacy\n",
        "import medspacy\n",
        "nlp = medspacy.load(enable=[\"sectionizer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKhs5OeuEtVb"
      },
      "outputs": [],
      "source": [
        "import medspacy\n",
        "from medspacy.section_detection import Sectionizer\n",
        "\n",
        "nlp = medspacy.load() # Load the basic medspacy model\n",
        "sectionizer = Sectionizer(nlp) # Create the sectionizer\n",
        "nlp.add_pipe(\"medspacy_sectionizer\")  # Explicitly add to pipeline using the full name\n",
        "\n",
        "from medspacy.section_detection import SectionRule\n",
        "\n",
        "rules = [\n",
        "    SectionRule(\"Social History:\", category=\"social_history\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARsCjisnF5JX"
      },
      "outputs": [],
      "source": [
        "import medspacy\n",
        "from medspacy.section_detection import Sectionizer\n",
        "\n",
        "nlp = medspacy.load()  # Load the basic medspacy model\n",
        "#sectionizer = Sectionizer(nlp)  # Create the sectionizer\n",
        "sectionizer = nlp.add_pipe(\"medspacy_sectionizer\")  # Explicitly add to pipeline using the full name\n",
        "\n",
        "from medspacy.section_detection import SectionRule\n",
        "\n",
        "rules = [\n",
        "    SectionRule(\"SOCIAL HISTORY:\", category=\"social_history\")\n",
        "]\n",
        "\n",
        "#sectionizer = nlp.get_pipe(\"medspacy_sectionizer\")\n",
        "sectionizer.add(rules)\n",
        "\n",
        "\n",
        "\n",
        "doc = nlp(\"\"\"\n",
        "PAST MEDICAL HISTORY:  Hypertension, fibromyalgia,\n",
        "hypothyroidism, NASH and noninsulin dependent diabetes.\n",
        "\n",
        "PAST SURGICAL HISTORY:  Hysterectomy and cholecystectomy.\n",
        "\n",
        "SOCIAL HISTORY:  She smokes a pack per day.\n",
        "\n",
        "\"\"\")\n",
        "for ent in doc.ents:\n",
        "    print(ent)\n",
        "    print(ent._.section_category)\n",
        "    print()\n",
        "\n",
        "\n",
        "for section in doc._.sections:\n",
        "    if section.category == \"social_history\":\n",
        "        # Get the text using doc.text and the start and end characters\n",
        "        extracted_text = doc.text[section.section_span[0]:section.section_span[1]]\n",
        "        print(f\"Extracted Text: {extracted_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzqLo269SZ66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVH2HIBRGIT8"
      },
      "outputs": [],
      "source": [
        "print(doc.text[3:12])  # Use slicing instead of substring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx1vHzWDAy7N",
        "outputId": "edfc6688-4609-4075-f63d-754bff6ec80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2M-TzpDZPN"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Prepare the loaded dataframes for further analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDCWMawxDaBX"
      },
      "source": [
        "**Reasoning**:\n",
        "Handle missing values and check data types for df_keywords and df_sbdh.  Since df_notes is empty, skip processing it.  Also, check if 'row_id' exists and has the same data type across the two dataframes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE5iOCYADaRC",
        "outputId": "b5cd84e8-d139-46cd-f145-f19355bff420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in df_keywords:\n",
            " row_id    0\n",
            "sbdh      0\n",
            "start     0\n",
            "end       0\n",
            "dtype: int64\n",
            "\n",
            "Data types in df_keywords:\n",
            " row_id     int64\n",
            "sbdh      object\n",
            "start      int64\n",
            "end        int64\n",
            "dtype: object\n",
            "\n",
            "Missing values in df_sbdh:\n",
            " row_id                    0\n",
            "sdoh_community_present    0\n",
            "sdoh_community_absent     0\n",
            "sdoh_education            0\n",
            "sdoh_economics            0\n",
            "sdoh_environment          0\n",
            "behavior_alcohol          0\n",
            "behavior_tobacco          0\n",
            "behavior_drug             0\n",
            "dtype: int64\n",
            "\n",
            "Data types in df_sbdh:\n",
            " row_id                    int64\n",
            "sdoh_community_present    int64\n",
            "sdoh_community_absent     int64\n",
            "sdoh_education            int64\n",
            "sdoh_economics            int64\n",
            "sdoh_environment          int64\n",
            "behavior_alcohol          int64\n",
            "behavior_tobacco          int64\n",
            "behavior_drug             int64\n",
            "dtype: object\n",
            "\n",
            "Data type of 'row_id' in df_keywords: int64\n",
            "Data type of 'row_id' in df_sbdh: int64\n",
            "'row_id' data types are consistent\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in df_keywords\n",
        "print(\"Missing values in df_keywords:\\n\", df_keywords.isnull().sum())\n",
        "\n",
        "# Check data types in df_keywords\n",
        "print(\"\\nData types in df_keywords:\\n\", df_keywords.dtypes)\n",
        "\n",
        "# Check for missing values in df_sbdh\n",
        "print(\"\\nMissing values in df_sbdh:\\n\", df_sbdh.isnull().sum())\n",
        "\n",
        "# Check data types in df_sbdh\n",
        "print(\"\\nData types in df_sbdh:\\n\", df_sbdh.dtypes)\n",
        "\n",
        "# Check 'row_id' data type consistency\n",
        "if {'row_id'}.issubset(df_keywords.columns) and {'row_id'}.issubset(df_sbdh.columns):\n",
        "    print(f\"\\nData type of 'row_id' in df_keywords: {df_keywords['row_id'].dtype}\")\n",
        "    print(f\"Data type of 'row_id' in df_sbdh: {df_sbdh['row_id'].dtype}\")\n",
        "    if df_keywords['row_id'].dtype != df_sbdh['row_id'].dtype:\n",
        "        print(\"Warning: 'row_id' data types are inconsistent.\")\n",
        "        # Convert to a common type (e.g., int64)\n",
        "        df_keywords['row_id'] = df_keywords['row_id'].astype('int64')\n",
        "        df_sbdh['row_id'] = df_sbdh['row_id'].astype('int64')\n",
        "        print(\"Converted 'row_id' to int64.\")\n",
        "    else:\n",
        "      print(\"'row_id' data types are consistent\")\n",
        "else:\n",
        "    print(\"Warning: 'row_id' column not found in both dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEFlUrbmDdm4"
      },
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded DataFrames (`df_keywords`, `df_sbdh`) to gain insights into their structure, data types, and statistical summaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx8N1m4IDeZq"
      },
      "source": [
        "**Reasoning**:\n",
        "Explore the dataframes using head, info, describe, and value_counts methods as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcCueJr4Dimi"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### 1. Q&A\n",
        "\n",
        "No questions were explicitly asked in the task prompt.  However, the provided analysis implicitly addresses questions like:\n",
        "\n",
        "* What is the structure of the data in each CSV file?\n",
        "* Are there any missing values in the data?\n",
        "* What are the data types of each column?\n",
        "* What is the distribution of the different SDOH and behavioral factors?\n",
        "* Is there any inconsistency in the 'row_id' column between the two dataframes?\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "\n",
        "* **Missing Data:** No missing values were found in `df_keywords` or `df_sbdh`.  `df_notes` could not be loaded due to a `FileNotFoundError`.\n",
        "* **`df_keywords` 'sbdh' Distribution:** The 'sdoh_community' category is the most frequent keyword (implied by the value_counts() output, though the exact count is not given in the result).\n",
        "* **`df_sbdh` SDOH/Behavior Distributions:**  'sdoh_community_present' and 'sdoh_environment' show a higher proportion of '1' values, suggesting a higher prevalence of these factors.  'behavior_alcohol' and 'behavior_tobacco' exhibit a wider range of values than 'behavior_drug', which is skewed towards '0' values (again, without exact counts).\n",
        "* **`row_id` consistency**: The `row_id` column in both `df_keywords` and `df_sbdh` has a consistent data type (int64).\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "\n",
        "* **Investigate Missing `NOTEEVENTS.csv`:** Determine why \"NOTEEVENTS.csv\" could not be loaded.  Check the file path, file permissions, and file existence.  This file may contain crucial information needed for the full analysis.\n",
        "* **Explore Relationships between DataFrames:** Since `df_keywords` and `df_sbdh` appear to be related through the `row_id`, investigate the relationships between the keywords identified in `df_keywords` and the SDOH/behavior factors in `df_sbdh`.  This could involve merging the two DataFrames and further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKlG-DJzDepV"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of each DataFrame\n",
        "display(df_keywords.head())\n",
        "display(df_sbdh.head())\n",
        "\n",
        "# Get a concise summary of each DataFrame\n",
        "display(df_keywords.info())\n",
        "display(df_sbdh.info())\n",
        "\n",
        "# Generate descriptive statistics for each DataFrame\n",
        "display(df_keywords.describe())\n",
        "display(df_sbdh.describe())\n",
        "\n",
        "# Examine the distribution of the 'sbdh' column in df_keywords\n",
        "display(df_keywords['sbdh'].value_counts())\n",
        "\n",
        "# Examine the distributions of SDOH/behavior columns in df_sbdh\n",
        "for col in ['sdoh_community_present', 'sdoh_community_absent', 'sdoh_education', 'sdoh_economics', 'sdoh_environment', 'behavior_alcohol', 'behavior_tobacco', 'behavior_drug']:\n",
        "    display(df_sbdh[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFea1MmSJONW"
      },
      "source": [
        "# Class Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OWifbLtFofH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Table 2: Class distribution for Social Determinants of Health\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_sbdh is already loaded\n",
        "\n",
        "# Select the relevant columns for SDOH\n",
        "sdoh_cols = ['sdoh_community_present', 'sdoh_community_absent', 'sdoh_education', 'sdoh_economics', 'sdoh_environment']\n",
        "\n",
        "# Create a dictionary to store the class distribution\n",
        "sdoh_distribution = {}\n",
        "\n",
        "# Iterate through each SDOH column and calculate the class distribution\n",
        "for col in sdoh_cols:\n",
        "    # Get the value counts for the column, including NaN values\n",
        "    value_counts = df_sbdh[col].value_counts(dropna=False).to_dict()\n",
        "\n",
        "    # Rename NaN to 'None' if present\n",
        "    if pd.isnull(df_sbdh[col]).any():\n",
        "        if float('nan') in value_counts:\n",
        "            value_counts['None'] = value_counts.pop(float('nan'))\n",
        "\n",
        "    # Store the class distribution for the current SDOH column\n",
        "    sdoh_distribution[col.replace('sdoh_', '').replace('_', '-').title()] = value_counts\n",
        "\n",
        "# Create a DataFrame from the class distribution dictionary\n",
        "table2 = pd.DataFrame.from_dict(sdoh_distribution, orient='index').fillna(0).astype(int)  # Fill NaN with 0 and convert to int\n",
        "\n",
        "# Reorder columns if necessary and rename for clarity\n",
        "if 'True' in table2.columns and 'False' in table2.columns and 'None' in table2.columns:\n",
        "  table2 = table2[['True', 'False', 'None']]  # Reorder columns\n",
        "else:\n",
        "  # If True/False not present, assume 1/0 and rename\n",
        "  table2 = table2.rename(columns={1: 'True', 0: 'False'})\n",
        "  if 'None' not in table2.columns:\n",
        "    table2['None'] = 0  # Add None column if not present\n",
        "\n",
        "# Display the table\n",
        "display(table2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4m2aZRSFlip"
      },
      "outputs": [],
      "source": [
        "# Select the relevant columns for behavioral factors\n",
        "behavior_cols = ['behavior_alcohol', 'behavior_tobacco', 'behavior_drug']\n",
        "\n",
        "column_mapping = {\n",
        "    3: 'Never',\n",
        "    1: 'Present',\n",
        "    0: 'None',  # Assuming 0 represents 'None'\n",
        "    4: 'Unsure',\n",
        "    2: 'Past'\n",
        "}\n",
        "\n",
        "# Create a dictionary to store the class distribution\n",
        "behavior_distribution = {}\n",
        "\n",
        "# Iterate through each behavior column and calculate the class distribution\n",
        "for col in behavior_cols:\n",
        "    # Get the value counts for the column, including NaN values\n",
        "    value_counts = df_sbdh[col].value_counts(dropna=False).to_dict() # dropna=False includes NaN\n",
        "\n",
        "    # Rename NaN to 'None' if present\n",
        "    if pd.isnull(df_sbdh[col]).any():  # Explicitly check for NaN using pd.isnull\n",
        "        if float('nan') in value_counts:\n",
        "            value_counts['None'] = value_counts.pop(float('nan'))  # Replace NaN key with 'None'\n",
        "\n",
        "    # Store the class distribution for the current behavior column\n",
        "    behavior_distribution[col.replace('behavior_', '').title()] = value_counts\n",
        "\n",
        "# Create a DataFrame from the class distribution dictionary\n",
        "table3 = pd.DataFrame.from_dict(behavior_distribution, orient='index').fillna('N/A').astype(object)\n",
        "table3 = table3.rename(columns=column_mapping)\n",
        "desired_order = ['Present', 'Past', 'Never', 'Unsure', 'None']\n",
        "table3 = table3.reindex(columns=[col for col in desired_order if col in table3.columns] +\n",
        "                          [col for col in table3.columns if col not in desired_order])\n",
        "# Display the table\n",
        "display(table3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCYeXjqJRpB"
      },
      "source": [
        "# Model - Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywrguh5aeU52"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Text Preprocessing:\n",
        "Converts text to lowercase\n",
        "Removes non-alphanumeric characters\n",
        "Removes stopwords using NLTK's stopwords list\n",
        "Tokenizes text\n",
        "Feature Engineering:\n",
        "Uses scikit-learn's TfidfVectorizer for:\n",
        "Bag-of-words representation\n",
        "TF-IDF feature calculation\n",
        "Vocabulary management\n",
        "Handles sparse matrices efficiently\n",
        "Added Features:\n",
        "Feature importance analysis\n",
        "Proper handling of train-test split to prevent data leakage\n",
        "Parallel processing for Random Forest```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlsMY4qrb2_t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHDataLoader:\n",
        "    def __init__(self, data_path):\n",
        "        \"\"\"Initialize the data loader\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the Excel/CSV file containing SBDH data\n",
        "        \"\"\"\n",
        "        self.data = pd.read_excel(data_path) if data_path.endswith('.xlsx') else pd.read_csv(data_path)\n",
        "\n",
        "        # Define the SBDH columns\n",
        "        self.sbdh_columns = [\n",
        "            'sdoh_community_present',\n",
        "            'sdoh_community_absent',\n",
        "            'sdoh_education',\n",
        "            'sdoh_economics',\n",
        "            'sdoh_environment',\n",
        "            'behavior_alcohol',\n",
        "            'behavior_tobacco',\n",
        "            'behavior_drug'\n",
        "        ]\n",
        "\n",
        "    def get_data_splits(self, target_column, test_size=0.2, random_state=42):\n",
        "        \"\"\"Get train-test splits for a specific SBDH target\n",
        "\n",
        "        Args:\n",
        "            target_column (str): Name of the target SBDH column\n",
        "            test_size (float): Proportion of data to use for testing\n",
        "            random_state (int): Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing (X_train, X_test, y_train, y_test)\n",
        "        \"\"\"\n",
        "        # Create feature matrix X by dropping row_id and target column\n",
        "        feature_columns = [col for col in self.sbdh_columns if col != target_column]\n",
        "        X = self.data[feature_columns]\n",
        "        y = self.data[target_column]\n",
        "\n",
        "        return train_test_split(X, y, test_size=test_size, random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1agj5zucLzau"
      },
      "source": [
        "## 1. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbeI2A-VnzQ"
      },
      "source": [
        "Class Imbalance Handling: The paper mentions using oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuElrapTVubC"
      },
      "source": [
        "Hyperparameter Tuning: The paper mentions tuning hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nKNZVpwWlSJ",
        "outputId": "ea6942fe-a842-435a-8338-04254d638e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
            "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: sklearn-compat, imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SlOqdoCXOAR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class SBDHRandomForest:\n",
        "    def __init__(self, n_estimators=100, max_features='sqrt', min_samples_split=2):\n",
        "        \"\"\"Initialize the Random Forest model with expected scores from paper\"\"\"\n",
        "        # Define parameter grid for tuning\n",
        "        self.param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_features': ['sqrt', 'log2'],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_features=max_features,\n",
        "            min_samples_split=min_samples_split,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # Expected scores from paper\n",
        "        self.expected_scores = {\n",
        "            'sdoh_community_present': (0.9325, 0.0085),\n",
        "            'sdoh_community_absent': (0.8686, 0.0199),\n",
        "            'sdoh_education': (0.8249, 0.0323),\n",
        "            'sdoh_economics': (0.8886, 0.0143),\n",
        "            'sdoh_environment': (0.9069, 0.0284),\n",
        "            'behavior_alcohol': (0.7071, 0.0173),\n",
        "            'behavior_tobacco': (0.7368, 0.0176),\n",
        "            'behavior_drug': (0.6697, 0.0075)\n",
        "        }\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        \"\"\"Train with hyperparameter tuning\"\"\"\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=self.model,\n",
        "            param_grid=self.param_grid,\n",
        "            cv=3,\n",
        "            scoring='f1_macro',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        self.model = grid_search.best_estimator_\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Basic evaluation using macro F1 score\"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "        return f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    def evaluate_with_cv(self, X, y, target_name):\n",
        "        \"\"\"Evaluate using 5-fold cross validation and compare with paper results\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Labels\n",
        "            target_name: Name of the SBDH target\n",
        "\n",
        "        Returns:\n",
        "            Dict containing mean F1, std, expected scores, and statistical significance\n",
        "        \"\"\"\n",
        "        # 5-fold cross validation\n",
        "        \"\"\"Evaluate using 5-fold cross validation with oversampling\"\"\"\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        f1_scores = []\n",
        "\n",
        "        # Initialize oversampler\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Perform cross validation\n",
        "        for train_idx, test_idx in kf.split(X):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "            # Apply oversampling to training data only\n",
        "            X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "            self.train(X_train_resampled, y_train_resampled)\n",
        "            f1 = self.evaluate(X_test, y_test)\n",
        "            f1_scores.append(f1)\n",
        "        # Calculate mean and standard deviation\n",
        "        mean_f1 = np.mean(f1_scores)\n",
        "        std_f1 = np.std(f1_scores)\n",
        "\n",
        "        # Get expected scores from paper\n",
        "        expected_mean, expected_std = self.expected_scores[target_name]\n",
        "        # Perform statistical significance test\n",
        "        t_stat, p_value = stats.ttest_1samp(f1_scores, expected_mean)\n",
        "\n",
        "        # Check if this is the best performing model for this target\n",
        "        is_best = False\n",
        "        if target_name == 'sdoh_environment': # from paper, # RF performed best for Environment\n",
        "            is_best = True\n",
        "\n",
        "        return {\n",
        "            'score': f\"{mean_f1:.4f} ± {std_f1:.4f}{'*' if is_best else ''}\",\n",
        "            'expected': f\"{expected_mean:.4f} ± {expected_std:.4f}\",\n",
        "            'p_value': p_value,\n",
        "            'raw_score': mean_f1,\n",
        "            'raw_std': std_f1\n",
        "        }\n",
        "\n",
        "    def get_feature_importance(self, feature_names):\n",
        "        \"\"\"Get feature importance scores\"\"\"\n",
        "        importances = self.model.feature_importances_\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importances\n",
        "        })\n",
        "        return feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "def train_all_sbdh_models(data_loader):\n",
        "    \"\"\"Train and evaluate models for all SBDH targets\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for target in data_loader.sbdh_columns:\n",
        "        print(f\"\\nEvaluating model for {target}\")\n",
        "\n",
        "        # Prepare features and target\n",
        "        X = data_loader.data[[col for col in data_loader.sbdh_columns if col != target]]\n",
        "        y = data_loader.data[target]\n",
        "\n",
        "        # Initialize and evaluate model\n",
        "        rf_model = SBDHRandomForest()\n",
        "        result = rf_model.evaluate_with_cv(X, y, target)\n",
        "        results[target] = result\n",
        "\n",
        "        print(f\"Obtained: {result['score']}\")\n",
        "        print(f\"Expected: {result['expected']}\")\n",
        "        print(f\"p-value: {result['p_value']:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_oWTiKZfyVa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Read input and output data\n",
        "input_df = pd.read_excel('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "output_df = pd.read_excel('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "\n",
        "# Merge the dataframes on row_id\n",
        "df = pd.merge(input_df, output_df, on='row_id')\n",
        "\n",
        "def train_rf_for_class(texts, labels):\n",
        "    # Convert text to TF-IDF features\n",
        "    vectorizer = TfidfVectorizer(lowercase=True,\n",
        "                                stop_words='english',\n",
        "                                token_pattern=r'[a-zA-Z]+')\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    # Initialize Random Forest classifier\n",
        "    rf = RandomForestClassifier(n_estimators=100,\n",
        "                               random_state=42)\n",
        "\n",
        "    # Perform 5-fold cross validation\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(rf, X, labels,\n",
        "                           scoring='f1_macro',\n",
        "                           cv=cv)\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "# Train separate classifiers for each SBDH\n",
        "sbdh_columns = ['sdoh_community_present', 'sdoh_community_absent',\n",
        "                'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "                'behavior_alcohol', 'behavior_tobacco', 'behavior_drug']\n",
        "\n",
        "results = {}\n",
        "for column in sbdh_columns:\n",
        "    X = df['Social_History']\n",
        "    y = df[column]\n",
        "    mean_score, std_score = train_rf_for_class(X, y)\n",
        "    results[column] = (mean_score, std_score)\n",
        "\n",
        "# Print results in table format\n",
        "print(\"\\nRandom Forest Macro-F1 scores:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'SBDH':<20} {'Macro-F1 Score':<20}\")\n",
        "print(\"-\" * 50)\n",
        "for sbdh, (mean, std) in results.items():\n",
        "    print(f\"{sbdh:<20} {f'{mean:.4f} ± {std:.4f}':<20}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FmJs_hX-0xkK",
        "outputId": "f7d5754a-2d0d-46ec-f56c-09484b5aeb08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Preprocessing texts...\n",
            "\n",
            "Processing sdoh_community_present...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.9181\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.9184\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.9195\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.9144\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.9218\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Processing sdoh_community_absent...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.6210\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.5904\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.5904\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.5818\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.6295\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Processing sdoh_education...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.5801\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.6538\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.5596\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.6185\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.6365\n",
            "Best parameters: {'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 50}\n",
            "\n",
            "Processing sdoh_economics...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.8463\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.8587\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.8544\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.8273\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.8424\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Processing sdoh_environment...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.6821\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.7268\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.6746\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.6606\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.7237\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Processing behavior_alcohol...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.6313\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.6931\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.6306\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.6735\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.6640\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Processing behavior_tobacco...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.7335\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.7260\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.7355\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.7696\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.7372\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Processing behavior_drug...\n",
            "\n",
            "Tuning hyperparameters for fold 1...\n",
            "Fold 1 Macro-F1: 0.5138\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 2...\n",
            "Fold 2 Macro-F1: 0.4671\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 3...\n",
            "Fold 3 Macro-F1: 0.4603\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 4...\n",
            "Fold 4 Macro-F1: 0.4913\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Tuning hyperparameters for fold 5...\n",
            "Fold 5 Macro-F1: 0.4853\n",
            "Best parameters: {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Final Results:\n",
            "------------------------------------------------------------\n",
            "SBDH                      Macro-F1 Score                     \n",
            "------------------------------------------------------------\n",
            "sdoh_community_present    0.9184 ± 0.0024                    \n",
            "sdoh_community_absent     0.6026 ± 0.0189                    \n",
            "sdoh_education            0.6097 ± 0.0350                    \n",
            "sdoh_economics            0.8458 ± 0.0109                    \n",
            "sdoh_environment          0.6936 ± 0.0268                    \n",
            "behavior_alcohol          0.6585 ± 0.0244                    \n",
            "behavior_tobacco          0.7403 ± 0.0151                    \n",
            "behavior_drug             0.4836 ± 0.0189                    \n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"SBDH\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"sdoh_community_absent\",\n          \"behavior_alcohol\",\n          \"sdoh_community_present\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro-F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13967494895771407,\n        \"min\": 0.48358590107941096,\n        \"max\": 0.9184295526235762,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.6026158287615438,\n          0.6585010822039787,\n          0.9184295526235762\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro-F1 Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010016450587582524,\n        \"min\": 0.0024099693310625932,\n        \"max\": 0.03501590492837238,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.018924003064775527,\n          0.024369315891793412,\n          0.0024099693310625932\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Best Parameters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-802d3af2-d971-4181-b69b-381858b65cc8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SBDH</th>\n",
              "      <th>Macro-F1 Mean</th>\n",
              "      <th>Macro-F1 Std</th>\n",
              "      <th>Best Parameters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sdoh_community_present</td>\n",
              "      <td>0.918430</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>[{'max_features': 'sqrt', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sdoh_community_absent</td>\n",
              "      <td>0.602616</td>\n",
              "      <td>0.018924</td>\n",
              "      <td>[{'max_features': 'log2', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sdoh_education</td>\n",
              "      <td>0.609708</td>\n",
              "      <td>0.035016</td>\n",
              "      <td>[{'max_features': 'log2', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sdoh_economics</td>\n",
              "      <td>0.845843</td>\n",
              "      <td>0.010895</td>\n",
              "      <td>[{'max_features': 'sqrt', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sdoh_environment</td>\n",
              "      <td>0.693561</td>\n",
              "      <td>0.026807</td>\n",
              "      <td>[{'max_features': 'sqrt', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>behavior_alcohol</td>\n",
              "      <td>0.658501</td>\n",
              "      <td>0.024369</td>\n",
              "      <td>[{'max_features': 'sqrt', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>behavior_tobacco</td>\n",
              "      <td>0.740347</td>\n",
              "      <td>0.015112</td>\n",
              "      <td>[{'max_features': 'sqrt', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>behavior_drug</td>\n",
              "      <td>0.483586</td>\n",
              "      <td>0.018914</td>\n",
              "      <td>[{'max_features': 'sqrt', 'min_samples_split':...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-802d3af2-d971-4181-b69b-381858b65cc8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-802d3af2-d971-4181-b69b-381858b65cc8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-802d3af2-d971-4181-b69b-381858b65cc8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cc07d427-84b1-4c80-bfe0-6e610016f984\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc07d427-84b1-4c80-bfe0-6e610016f984')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cc07d427-84b1-4c80-bfe0-6e610016f984 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_aa47afc8-d4dc-48d1-b212-27e2803a4a2b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_aa47afc8-d4dc-48d1-b212-27e2803a4a2b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                     SBDH  Macro-F1 Mean  Macro-F1 Std  \\\n",
              "0  sdoh_community_present       0.918430      0.002410   \n",
              "1   sdoh_community_absent       0.602616      0.018924   \n",
              "2          sdoh_education       0.609708      0.035016   \n",
              "3          sdoh_economics       0.845843      0.010895   \n",
              "4        sdoh_environment       0.693561      0.026807   \n",
              "5        behavior_alcohol       0.658501      0.024369   \n",
              "6        behavior_tobacco       0.740347      0.015112   \n",
              "7           behavior_drug       0.483586      0.018914   \n",
              "\n",
              "                                     Best Parameters  \n",
              "0  [{'max_features': 'sqrt', 'min_samples_split':...  \n",
              "1  [{'max_features': 'log2', 'min_samples_split':...  \n",
              "2  [{'max_features': 'log2', 'min_samples_split':...  \n",
              "3  [{'max_features': 'sqrt', 'min_samples_split':...  \n",
              "4  [{'max_features': 'sqrt', 'min_samples_split':...  \n",
              "5  [{'max_features': 'sqrt', 'min_samples_split':...  \n",
              "6  [{'max_features': 'sqrt', 'min_samples_split':...  \n",
              "7  [{'max_features': 'sqrt', 'min_samples_split':...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHClassifier:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=True,\n",
        "            token_pattern=r'[a-zA-Z]+',\n",
        "            stop_words='english'\n",
        "        )\n",
        "        # Initialize RF with default parameters\n",
        "        self.rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove de-identified parts [**...**]\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tune_hyperparameters(self, X_train, y_train):\n",
        "        # Define parameter grid for RF as mentioned in the paper\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200],  # number of learners\n",
        "            'max_features': ['sqrt', 'log2'],  # number of features\n",
        "            'min_samples_split': [2, 5, 10]  # minimum samples required to split\n",
        "        }\n",
        "\n",
        "        # Initialize GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=RandomForestClassifier(random_state=42),\n",
        "            param_grid=param_grid,\n",
        "            cv=5,\n",
        "            scoring='f1_macro',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Update RF with best parameters\n",
        "        self.rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)\n",
        "\n",
        "        return grid_search.best_params_\n",
        "\n",
        "    def train_evaluate(self, X, y):\n",
        "        # Initialize cross-validation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        fold_scores = []\n",
        "        best_params_per_fold = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "            # Split data\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Transform text to TF-IDF features\n",
        "            X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
        "            X_val_tfidf = self.vectorizer.transform(X_val)\n",
        "\n",
        "            # Handle class imbalance\n",
        "            sampler = RandomOverSampler(random_state=42)\n",
        "            X_train_balanced, y_train_balanced = sampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "            # Tune hyperparameters\n",
        "            print(f\"\\nTuning hyperparameters for fold {fold}...\")\n",
        "            best_params = self.tune_hyperparameters(X_train_balanced, y_train_balanced)\n",
        "            best_params_per_fold.append(best_params)\n",
        "\n",
        "            # Train model with tuned parameters\n",
        "            self.rf.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = self.rf.predict(X_val_tfidf)\n",
        "            fold_score = f1_score(y_val, y_pred, average='macro')\n",
        "            fold_scores.append(fold_score)\n",
        "\n",
        "            print(f\"Fold {fold} Macro-F1: {fold_score:.4f}\")\n",
        "            print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        std_score = np.std(fold_scores)\n",
        "        return mean_score, std_score, best_params_per_fold\n",
        "\n",
        "# Load data from Google Drive\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "\n",
        "# Merge dataframes\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = SBDHClassifier()\n",
        "\n",
        "# Preprocess all texts\n",
        "print(\"Preprocessing texts...\")\n",
        "X = np.array([classifier.preprocess_text(text) for text in df['Social_History']])\n",
        "\n",
        "# SBDH categories to process\n",
        "sbdh_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent',\n",
        "    'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "    'behavior_alcohol', 'behavior_tobacco', 'behavior_drug'\n",
        "]\n",
        "\n",
        "# Train and evaluate for each SBDH\n",
        "results = {}\n",
        "all_best_params = {}\n",
        "for sbdh in sbdh_columns:\n",
        "    print(f\"\\nProcessing {sbdh}...\")\n",
        "    y = np.array(df[sbdh])\n",
        "    mean_score, std_score, best_params = classifier.train_evaluate(X, y)\n",
        "    results[sbdh] = (mean_score, std_score)\n",
        "    all_best_params[sbdh] = best_params\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'SBDH':<25} {'Macro-F1 Score':<35}\")\n",
        "print(\"-\" * 60)\n",
        "for sbdh, (mean, std) in results.items():\n",
        "    print(f\"{sbdh:<25} {f'{mean:.4f} ± {std:.4f}':<35}\")\n",
        "\n",
        "# Create a DataFrame with results\n",
        "results_df = pd.DataFrame([\n",
        "    {'SBDH': sbdh,\n",
        "     'Macro-F1 Mean': mean,\n",
        "     'Macro-F1 Std': std,\n",
        "     'Best Parameters': all_best_params[sbdh]}\n",
        "    for sbdh, (mean, std) in results.items()\n",
        "])\n",
        "\n",
        "# Display results\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS5S0ZQx9WNR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removed Oversampling and hyperparameter tuning"
      ],
      "metadata": {
        "id": "hkALh2Zmu4B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHClassifier:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=True,\n",
        "            token_pattern=r'[a-zA-Z]+',\n",
        "            stop_words='english'\n",
        "        )\n",
        "        # Initialize RF with fixed parameters\n",
        "        self.rf = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_features='sqrt',\n",
        "            min_samples_split=2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove de-identified parts [**...**]\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def train_evaluate(self, X, y):\n",
        "        # Initialize cross-validation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        fold_scores = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "            # Split data\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Transform text to TF-IDF features\n",
        "            X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
        "            X_val_tfidf = self.vectorizer.transform(X_val)\n",
        "\n",
        "            # Train model\n",
        "            self.rf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = self.rf.predict(X_val_tfidf)\n",
        "            fold_score = f1_score(y_val, y_pred, average='macro')\n",
        "            fold_scores.append(fold_score)\n",
        "\n",
        "            print(f\"Fold {fold} Macro-F1: {fold_score:.4f}\")\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        std_score = np.std(fold_scores)\n",
        "        return mean_score, std_score\n",
        "\n",
        "# Load data from Google Drive\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "\n",
        "# Merge dataframes\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = SBDHClassifier()\n",
        "\n",
        "# Preprocess all texts\n",
        "print(\"Preprocessing texts...\")\n",
        "X = np.array([classifier.preprocess_text(text) for text in df['Social_History']])\n",
        "\n",
        "# SBDH categories to process\n",
        "sbdh_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent',\n",
        "    'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "    'behavior_alcohol', 'behavior_tobacco', 'behavior_drug'\n",
        "]\n",
        "\n",
        "# Train and evaluate for each SBDH\n",
        "results = {}\n",
        "for sbdh in sbdh_columns:\n",
        "    print(f\"\\nProcessing {sbdh}...\")\n",
        "    # Print class distribution\n",
        "    print(\"Class distribution:\")\n",
        "    print(df[sbdh].value_counts())\n",
        "\n",
        "    y = np.array(df[sbdh])\n",
        "    mean_score, std_score = classifier.train_evaluate(X, y)\n",
        "    results[sbdh] = (mean_score, std_score)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'SBDH':<25} {'Macro-F1 Score':<35}\")\n",
        "print(\"-\" * 60)\n",
        "for sbdh, (mean, std) in results.items():\n",
        "    print(f\"{sbdh:<25} {f'{mean:.4f} ± {std:.4f}':<35}\")\n",
        "\n",
        "# Create a DataFrame with results\n",
        "results_df = pd.DataFrame([\n",
        "    {'SBDH': sbdh,\n",
        "     'Macro-F1 Mean': mean,\n",
        "     'Macro-F1 Std': std}\n",
        "    for sbdh, (mean, std) in results.items()\n",
        "])\n",
        "\n",
        "# Display results\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oUW6YFi1u9dF",
        "outputId": "d8e2449b-64bf-4b92-dd94-78728bc34bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Preprocessing texts...\n",
            "\n",
            "Processing sdoh_community_present...\n",
            "Class distribution:\n",
            "sdoh_community_present\n",
            "1    4463\n",
            "0    2562\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.9003\n",
            "Fold 2 Macro-F1: 0.8987\n",
            "Fold 3 Macro-F1: 0.9006\n",
            "Fold 4 Macro-F1: 0.9026\n",
            "Fold 5 Macro-F1: 0.8978\n",
            "\n",
            "Processing sdoh_community_absent...\n",
            "Class distribution:\n",
            "sdoh_community_absent\n",
            "0    6241\n",
            "1     784\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.8668\n",
            "Fold 2 Macro-F1: 0.8572\n",
            "Fold 3 Macro-F1: 0.8670\n",
            "Fold 4 Macro-F1: 0.8480\n",
            "Fold 5 Macro-F1: 0.8829\n",
            "\n",
            "Processing sdoh_education...\n",
            "Class distribution:\n",
            "sdoh_education\n",
            "0    6815\n",
            "1     210\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.7970\n",
            "Fold 2 Macro-F1: 0.8186\n",
            "Fold 3 Macro-F1: 0.7583\n",
            "Fold 4 Macro-F1: 0.7738\n",
            "Fold 5 Macro-F1: 0.8138\n",
            "\n",
            "Processing sdoh_economics...\n",
            "Class distribution:\n",
            "sdoh_economics\n",
            "0    4295\n",
            "2    1742\n",
            "1     988\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.8317\n",
            "Fold 2 Macro-F1: 0.8631\n",
            "Fold 3 Macro-F1: 0.8609\n",
            "Fold 4 Macro-F1: 0.8471\n",
            "Fold 5 Macro-F1: 0.8483\n",
            "\n",
            "Processing sdoh_environment...\n",
            "Class distribution:\n",
            "sdoh_environment\n",
            "1    4357\n",
            "0    2605\n",
            "2      63\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.7159\n",
            "Fold 2 Macro-F1: 0.8492\n",
            "Fold 3 Macro-F1: 0.7961\n",
            "Fold 4 Macro-F1: 0.7830\n",
            "Fold 5 Macro-F1: 0.8199\n",
            "\n",
            "Processing behavior_alcohol...\n",
            "Class distribution:\n",
            "behavior_alcohol\n",
            "3    2444\n",
            "1    2077\n",
            "0    1657\n",
            "2     515\n",
            "4     332\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.5957\n",
            "Fold 2 Macro-F1: 0.6428\n",
            "Fold 3 Macro-F1: 0.5871\n",
            "Fold 4 Macro-F1: 0.6216\n",
            "Fold 5 Macro-F1: 0.5986\n",
            "\n",
            "Processing behavior_tobacco...\n",
            "Class distribution:\n",
            "behavior_tobacco\n",
            "3    2252\n",
            "2    2121\n",
            "0    1291\n",
            "1    1006\n",
            "4     355\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.7203\n",
            "Fold 2 Macro-F1: 0.7280\n",
            "Fold 3 Macro-F1: 0.6927\n",
            "Fold 4 Macro-F1: 0.7321\n",
            "Fold 5 Macro-F1: 0.7141\n",
            "\n",
            "Processing behavior_drug...\n",
            "Class distribution:\n",
            "behavior_drug\n",
            "0    4317\n",
            "3    2136\n",
            "2     221\n",
            "1     207\n",
            "4     144\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.5003\n",
            "Fold 2 Macro-F1: 0.4242\n",
            "Fold 3 Macro-F1: 0.4546\n",
            "Fold 4 Macro-F1: 0.5098\n",
            "Fold 5 Macro-F1: 0.4639\n",
            "\n",
            "Final Results:\n",
            "------------------------------------------------------------\n",
            "SBDH                      Macro-F1 Score                     \n",
            "------------------------------------------------------------\n",
            "sdoh_community_present    0.9000 ± 0.0016                    \n",
            "sdoh_community_absent     0.8644 ± 0.0116                    \n",
            "sdoh_education            0.7923 ± 0.0231                    \n",
            "sdoh_economics            0.8502 ± 0.0113                    \n",
            "sdoh_environment          0.7928 ± 0.0445                    \n",
            "behavior_alcohol          0.6091 ± 0.0203                    \n",
            "behavior_tobacco          0.7174 ± 0.0138                    \n",
            "behavior_drug             0.4705 ± 0.0312                    \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     SBDH  Macro-F1 Mean  Macro-F1 Std\n",
              "0  sdoh_community_present       0.899978      0.001648\n",
              "1   sdoh_community_absent       0.864361      0.011645\n",
              "2          sdoh_education       0.792291      0.023142\n",
              "3          sdoh_economics       0.850224      0.011270\n",
              "4        sdoh_environment       0.792814      0.044547\n",
              "5        behavior_alcohol       0.609150      0.020319\n",
              "6        behavior_tobacco       0.717436      0.013813\n",
              "7           behavior_drug       0.470544      0.031238"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25e06ba3-4939-47fb-8115-6ca4f9561711\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SBDH</th>\n",
              "      <th>Macro-F1 Mean</th>\n",
              "      <th>Macro-F1 Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sdoh_community_present</td>\n",
              "      <td>0.899978</td>\n",
              "      <td>0.001648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sdoh_community_absent</td>\n",
              "      <td>0.864361</td>\n",
              "      <td>0.011645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sdoh_education</td>\n",
              "      <td>0.792291</td>\n",
              "      <td>0.023142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sdoh_economics</td>\n",
              "      <td>0.850224</td>\n",
              "      <td>0.011270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sdoh_environment</td>\n",
              "      <td>0.792814</td>\n",
              "      <td>0.044547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>behavior_alcohol</td>\n",
              "      <td>0.609150</td>\n",
              "      <td>0.020319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>behavior_tobacco</td>\n",
              "      <td>0.717436</td>\n",
              "      <td>0.013813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>behavior_drug</td>\n",
              "      <td>0.470544</td>\n",
              "      <td>0.031238</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25e06ba3-4939-47fb-8115-6ca4f9561711')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25e06ba3-4939-47fb-8115-6ca4f9561711 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25e06ba3-4939-47fb-8115-6ca4f9561711');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7a606e34-656c-45f3-a55c-a62a93ae282b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a606e34-656c-45f3-a55c-a62a93ae282b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7a606e34-656c-45f3-a55c-a62a93ae282b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ddfde5ae-9e43-47fb-a04e-7042c590e9cd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ddfde5ae-9e43-47fb-a04e-7042c590e9cd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"SBDH\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"sdoh_community_absent\",\n          \"behavior_alcohol\",\n          \"sdoh_community_present\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro-F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.145519544196374,\n        \"min\": 0.4705442286062732,\n        \"max\": 0.899977931026599,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8643610319818306,\n          0.609149947115012,\n          0.899977931026599\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro-F1 Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013412157028918147,\n        \"min\": 0.0016482065367585336,\n        \"max\": 0.04454694042027519,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.011644863190404501,\n          0.020318733925210955,\n          0.0016482065367585336\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf3X08DSL4S-"
      },
      "source": [
        "## 2. XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without Oversampling and Hyperparameter tuning"
      ],
      "metadata": {
        "id": "F3mcnhZsxcf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHClassifier:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=True,\n",
        "            token_pattern=r'[a-zA-Z]+',\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove de-identified parts [**...**]\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def train_evaluate(self, X, y):\n",
        "        # Initialize cross-validation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        fold_scores = []\n",
        "\n",
        "        # Get number of unique classes\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Initialize XGBoost with fixed parameters\n",
        "        self.xgb = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            reg_lambda=1.0,\n",
        "            objective='multi:softmax',\n",
        "            num_class=num_classes,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "            # Split data\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Transform text to TF-IDF features\n",
        "            X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
        "            X_val_tfidf = self.vectorizer.transform(X_val)\n",
        "\n",
        "            # Train model\n",
        "            self.xgb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = self.xgb.predict(X_val_tfidf)\n",
        "            fold_score = f1_score(y_val, y_pred, average='macro')\n",
        "            fold_scores.append(fold_score)\n",
        "\n",
        "            print(f\"Fold {fold} Macro-F1: {fold_score:.4f}\")\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        std_score = np.std(fold_scores)\n",
        "        return mean_score, std_score\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "\n",
        "# Merge dataframes\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = SBDHClassifier()\n",
        "\n",
        "# Preprocess all texts\n",
        "print(\"\\nPreprocessing texts...\")\n",
        "X = np.array([classifier.preprocess_text(text) for text in df['Social_History']])\n",
        "\n",
        "# SBDH categories to process\n",
        "sbdh_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent',\n",
        "    'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "    'behavior_alcohol', 'behavior_tobacco', 'behavior_drug'\n",
        "]\n",
        "\n",
        "# Train and evaluate for each SBDH\n",
        "results = {}\n",
        "for sbdh in sbdh_columns:\n",
        "    print(f\"\\nProcessing {sbdh}...\")\n",
        "    print(\"Class distribution:\")\n",
        "    print(df[sbdh].value_counts())\n",
        "\n",
        "    y = np.array(df[sbdh])\n",
        "    mean_score, std_score = classifier.train_evaluate(X, y)\n",
        "    results[sbdh] = (mean_score, std_score)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'SBDH':<25} {'Macro-F1 Score':<35}\")\n",
        "print(\"-\" * 60)\n",
        "for sbdh, (mean, std) in results.items():\n",
        "    print(f\"{sbdh:<25} {f'{mean:.4f} ± {std:.4f}':<35}\")\n",
        "\n",
        "# Create a DataFrame with results\n",
        "results_df = pd.DataFrame([\n",
        "    {'SBDH': sbdh,\n",
        "     'Macro-F1 Mean': mean,\n",
        "     'Macro-F1 Std': std}\n",
        "    for sbdh, (mean, std) in results.items()\n",
        "])\n",
        "\n",
        "# Display results\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUOF1z_MxdoD",
        "outputId": "2f8fd586-2c58-4953-83b2-8c3060e59c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Preprocessing texts...\n",
            "\n",
            "Processing sdoh_community_present...\n",
            "Class distribution:\n",
            "sdoh_community_present\n",
            "1    4463\n",
            "0    2562\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.9223\n",
            "Fold 2 Macro-F1: 0.9276\n",
            "Fold 3 Macro-F1: 0.9403\n",
            "Fold 4 Macro-F1: 0.9354\n",
            "Fold 5 Macro-F1: 0.9294\n",
            "\n",
            "Processing sdoh_community_absent...\n",
            "Class distribution:\n",
            "sdoh_community_absent\n",
            "0    6241\n",
            "1     784\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.9059\n",
            "Fold 2 Macro-F1: 0.9122\n",
            "Fold 3 Macro-F1: 0.8952\n",
            "Fold 4 Macro-F1: 0.9076\n",
            "Fold 5 Macro-F1: 0.9065\n",
            "\n",
            "Processing sdoh_education...\n",
            "Class distribution:\n",
            "sdoh_education\n",
            "0    6815\n",
            "1     210\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.9077\n",
            "Fold 2 Macro-F1: 0.8425\n",
            "Fold 3 Macro-F1: 0.8377\n",
            "Fold 4 Macro-F1: 0.8763\n",
            "Fold 5 Macro-F1: 0.8598\n",
            "\n",
            "Processing sdoh_economics...\n",
            "Class distribution:\n",
            "sdoh_economics\n",
            "0    4295\n",
            "2    1742\n",
            "1     988\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.8574\n",
            "Fold 2 Macro-F1: 0.8676\n",
            "Fold 3 Macro-F1: 0.8716\n",
            "Fold 4 Macro-F1: 0.8496\n",
            "Fold 5 Macro-F1: 0.8576\n",
            "\n",
            "Processing sdoh_environment...\n",
            "Class distribution:\n",
            "sdoh_environment\n",
            "1    4357\n",
            "0    2605\n",
            "2      63\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.8833\n",
            "Fold 2 Macro-F1: 0.9299\n",
            "Fold 3 Macro-F1: 0.8960\n",
            "Fold 4 Macro-F1: 0.9096\n",
            "Fold 5 Macro-F1: 0.8961\n",
            "\n",
            "Processing behavior_alcohol...\n",
            "Class distribution:\n",
            "behavior_alcohol\n",
            "3    2444\n",
            "1    2077\n",
            "0    1657\n",
            "2     515\n",
            "4     332\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.6703\n",
            "Fold 2 Macro-F1: 0.7118\n",
            "Fold 3 Macro-F1: 0.6819\n",
            "Fold 4 Macro-F1: 0.6949\n",
            "Fold 5 Macro-F1: 0.6820\n",
            "\n",
            "Processing behavior_tobacco...\n",
            "Class distribution:\n",
            "behavior_tobacco\n",
            "3    2252\n",
            "2    2121\n",
            "0    1291\n",
            "1    1006\n",
            "4     355\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.7666\n",
            "Fold 2 Macro-F1: 0.7733\n",
            "Fold 3 Macro-F1: 0.7635\n",
            "Fold 4 Macro-F1: 0.7732\n",
            "Fold 5 Macro-F1: 0.7570\n",
            "\n",
            "Processing behavior_drug...\n",
            "Class distribution:\n",
            "behavior_drug\n",
            "0    4317\n",
            "3    2136\n",
            "2     221\n",
            "1     207\n",
            "4     144\n",
            "Name: count, dtype: int64\n",
            "Fold 1 Macro-F1: 0.6525\n",
            "Fold 2 Macro-F1: 0.6362\n",
            "Fold 3 Macro-F1: 0.5997\n",
            "Fold 4 Macro-F1: 0.6559\n",
            "Fold 5 Macro-F1: 0.6302\n",
            "\n",
            "Final Results:\n",
            "------------------------------------------------------------\n",
            "SBDH                      Macro-F1 Score                     \n",
            "------------------------------------------------------------\n",
            "sdoh_community_present    0.9310 ± 0.0063                    \n",
            "sdoh_community_absent     0.9055 ± 0.0056                    \n",
            "sdoh_education            0.8648 ± 0.0254                    \n",
            "sdoh_economics            0.8608 ± 0.0079                    \n",
            "sdoh_environment          0.9030 ± 0.0158                    \n",
            "behavior_alcohol          0.6882 ± 0.0141                    \n",
            "behavior_tobacco          0.7667 ± 0.0062                    \n",
            "behavior_drug             0.6349 ± 0.0201                    \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     SBDH  Macro-F1 Mean  Macro-F1 Std\n",
              "0  sdoh_community_present       0.931012      0.006255\n",
              "1   sdoh_community_absent       0.905470      0.005582\n",
              "2          sdoh_education       0.864787      0.025413\n",
              "3          sdoh_economics       0.860764      0.007881\n",
              "4        sdoh_environment       0.902994      0.015842\n",
              "5        behavior_alcohol       0.688165      0.014133\n",
              "6        behavior_tobacco       0.766716      0.006160\n",
              "7           behavior_drug       0.634912      0.020076"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b82a8ce-4a61-43f7-b3a8-972cdc3efce2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SBDH</th>\n",
              "      <th>Macro-F1 Mean</th>\n",
              "      <th>Macro-F1 Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sdoh_community_present</td>\n",
              "      <td>0.931012</td>\n",
              "      <td>0.006255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sdoh_community_absent</td>\n",
              "      <td>0.905470</td>\n",
              "      <td>0.005582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sdoh_education</td>\n",
              "      <td>0.864787</td>\n",
              "      <td>0.025413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sdoh_economics</td>\n",
              "      <td>0.860764</td>\n",
              "      <td>0.007881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sdoh_environment</td>\n",
              "      <td>0.902994</td>\n",
              "      <td>0.015842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>behavior_alcohol</td>\n",
              "      <td>0.688165</td>\n",
              "      <td>0.014133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>behavior_tobacco</td>\n",
              "      <td>0.766716</td>\n",
              "      <td>0.006160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>behavior_drug</td>\n",
              "      <td>0.634912</td>\n",
              "      <td>0.020076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b82a8ce-4a61-43f7-b3a8-972cdc3efce2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2b82a8ce-4a61-43f7-b3a8-972cdc3efce2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2b82a8ce-4a61-43f7-b3a8-972cdc3efce2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3bc2e70d-6109-4069-a299-0d1c484e9c52\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3bc2e70d-6109-4069-a299-0d1c484e9c52')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3bc2e70d-6109-4069-a299-0d1c484e9c52 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3321b363-ca7c-4675-bb56-c9d7b09a053a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3321b363-ca7c-4675-bb56-c9d7b09a053a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"SBDH\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"sdoh_community_absent\",\n          \"behavior_alcohol\",\n          \"sdoh_community_present\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro-F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10997292131850198,\n        \"min\": 0.6349117594284309,\n        \"max\": 0.9310120174992143,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9054701587759414,\n          0.6881651870779707,\n          0.9310120174992143\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro-F1 Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007427113839655142,\n        \"min\": 0.005582189136823802,\n        \"max\": 0.025412654941229122,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.005582189136823802,\n          0.01413277325053068,\n          0.006254929200641108\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XBoost with tuning and oversampling"
      ],
      "metadata": {
        "id": "_RRLtL2Xd2Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHClassifier:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=True,\n",
        "            token_pattern=r'[a-zA-Z]+',\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove de-identified parts [**...**]\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def train_evaluate(self, X, y):\n",
        "        # Initialize cross-validation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        fold_scores = []\n",
        "\n",
        "        # Get number of unique classes\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Initialize XGBoost with correct number of classes\n",
        "        self.xgb = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            reg_lambda=1.0,\n",
        "            objective='multi:softmax',\n",
        "            num_class=num_classes,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "            # Split data\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Transform text to TF-IDF features\n",
        "            X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
        "            X_val_tfidf = self.vectorizer.transform(X_val)\n",
        "\n",
        "            # Handle class imbalance\n",
        "            sampler = RandomOverSampler(random_state=42)\n",
        "            X_train_balanced, y_train_balanced = sampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "            # Train model\n",
        "            self.xgb.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = self.xgb.predict(X_val_tfidf)\n",
        "            fold_score = f1_score(y_val, y_pred, average='macro')\n",
        "            fold_scores.append(fold_score)\n",
        "\n",
        "            print(f\"Fold {fold} Macro-F1: {fold_score:.4f}\")\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        std_score = np.std(fold_scores)\n",
        "        return mean_score, std_score\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "\n",
        "# Merge dataframes\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = SBDHClassifier()\n",
        "\n",
        "# Preprocess all texts\n",
        "print(\"\\nPreprocessing texts...\")\n",
        "X = np.array([classifier.preprocess_text(text) for text in df['Social_History']])\n",
        "\n",
        "# SBDH categories to process\n",
        "sbdh_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent',\n",
        "    'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "    'behavior_alcohol', 'behavior_tobacco', 'behavior_drug'\n",
        "]\n",
        "\n",
        "# Train and evaluate for each SBDH\n",
        "results = {}\n",
        "for sbdh in sbdh_columns:\n",
        "    print(f\"\\nProcessing {sbdh}...\")\n",
        "    print(\"Class distribution:\")\n",
        "    print(df[sbdh].value_counts())\n",
        "\n",
        "    y = np.array(df[sbdh])\n",
        "    mean_score, std_score = classifier.train_evaluate(X, y)\n",
        "    results[sbdh] = (mean_score, std_score)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'SBDH':<25} {'Macro-F1 Score':<35}\")\n",
        "print(\"-\" * 60)\n",
        "for sbdh, (mean, std) in results.items():\n",
        "    print(f\"{sbdh:<25} {f'{mean:.4f} ± {std:.4f}':<35}\")\n",
        "\n",
        "# Create a DataFrame with results\n",
        "results_df = pd.DataFrame([\n",
        "    {'SBDH': sbdh,\n",
        "     'Macro-F1 Mean': mean,\n",
        "     'Macro-F1 Std': std}\n",
        "    for sbdh, (mean, std) in results.items()\n",
        "])\n",
        "\n",
        "# Display results\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "5_mn2T0LdtMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MEX6eTUrd0_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SMOTE as additional extension"
      ],
      "metadata": {
        "id": "lTEv3dzjd57k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHClassifier:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=True,\n",
        "            token_pattern=r'[a-zA-Z]+',\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove de-identified parts [**...**]\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def train_evaluate(self, X, y):\n",
        "        # Initialize cross-validation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        fold_scores = []\n",
        "\n",
        "        # Get number of unique classes\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Initialize XGBoost with correct number of classes\n",
        "        self.xgb = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            reg_lambda=1.0,\n",
        "            objective='multi:softmax',\n",
        "            num_class=num_classes,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "            # Split data\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            # Transform text to TF-IDF features\n",
        "            X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
        "            X_val_tfidf = self.vectorizer.transform(X_val)\n",
        "\n",
        "            # Convert sparse matrix to dense for SMOTE\n",
        "            X_train_dense = X_train_tfidf.toarray()\n",
        "\n",
        "            # Handle class imbalance using SMOTE\n",
        "            smote = SMOTE(random_state=42)\n",
        "            X_train_balanced, y_train_balanced = smote.fit_resample(X_train_dense, y_train)\n",
        "\n",
        "            # Train model\n",
        "            self.xgb.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = self.xgb.predict(X_val_tfidf)\n",
        "            fold_score = f1_score(y_val, y_pred, average='macro')\n",
        "            fold_scores.append(fold_score)\n",
        "\n",
        "            print(f\"Fold {fold} Macro-F1: {fold_score:.4f}\")\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        std_score = np.std(fold_scores)\n",
        "        return mean_score, std_score\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "\n",
        "# Merge dataframes\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = SBDHClassifier()\n",
        "\n",
        "# Preprocess all texts\n",
        "print(\"\\nPreprocessing texts...\")\n",
        "X = np.array([classifier.preprocess_text(text) for text in df['Social_History']])\n",
        "\n",
        "# SBDH categories to process\n",
        "sbdh_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent',\n",
        "    'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "    'behavior_alcohol', 'behavior_tobacco', 'behavior_drug'\n",
        "]\n",
        "\n",
        "# Train and evaluate for each SBDH\n",
        "results = {}\n",
        "for sbdh in sbdh_columns:\n",
        "    print(f\"\\nProcessing {sbdh}...\")\n",
        "    print(\"Class distribution:\")\n",
        "    print(df[sbdh].value_counts())\n",
        "\n",
        "    y = np.array(df[sbdh])\n",
        "    mean_score, std_score = classifier.train_evaluate(X, y)\n",
        "    results[sbdh] = (mean_score, std_score)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'SBDH':<25} {'Macro-F1 Score':<35}\")\n",
        "print(\"-\" * 60)\n",
        "for sbdh, (mean, std) in results.items():\n",
        "    print(f\"{sbdh:<25} {f'{mean:.4f} ± {std:.4f}':<35}\")\n",
        "\n",
        "# Create a DataFrame with results\n",
        "results_df = pd.DataFrame([\n",
        "    {'SBDH': sbdh,\n",
        "     'Macro-F1 Mean': mean,\n",
        "     'Macro-F1 Std': std}\n",
        "    for sbdh, (mean, std) in results.items()\n",
        "])\n",
        "\n",
        "# Display results\n",
        "results_df"
      ],
      "metadata": {
        "id": "b7ow3PKXd852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FFgpYpJL7UO"
      },
      "source": [
        "## 3. Bio-Clinical BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIycJzwwOA_4",
        "outputId": "e47f4249-c7e6-48f8-a0a8-8b828c40f2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJqBcESIMhDQ",
        "outputId": "dc60411d-fc21-4471-96dd-b57a4ec3f15a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\n",
            "Training Bio-ClinicalBERT for sdoh_community_present\n",
            "\n",
            "Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU setup and memory check\n",
        "def check_gpu():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
        "        print(f\"Available GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"No GPU available!\")\n",
        "\n",
        "check_gpu()\n",
        "\n",
        "class SBDHDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        text = ' '.join(text.split()[:50])  # Truncate text for T4 memory\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class BioClinicalBERTClassifier:\n",
        "    def __init__(self, num_labels):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            'emilyalsentzer/Bio_ClinicalBERT',\n",
        "            num_labels=num_labels,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = torch.nn.DataParallel(self.model)\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        print(f\"Model initialized with {num_labels} labels\")\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=5):\n",
        "        batch_accumulation = 4\n",
        "\n",
        "        labels = torch.tensor([item['labels'] for item in train_loader.dataset])\n",
        "        class_counts = torch.bincount(labels)\n",
        "        total_samples = len(labels)\n",
        "        class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "        class_weights = class_weights.to(self.device)\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
        "\n",
        "        total_steps = len(train_loader) * epochs // batch_accumulation\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                if i % 100 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "                loss = criterion(outputs.logits, labels) / batch_accumulation\n",
        "                loss.backward()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                if (i + 1) % batch_accumulation == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                if i % 50 == 0:\n",
        "                    print(f'Epoch {epoch+1}, Step {i}, Loss: {loss.item():.4f}')\n",
        "                    print(f'GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
        "\n",
        "            avg_train_loss = total_train_loss * batch_accumulation / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            total_val_loss = 0\n",
        "            val_predictions = []\n",
        "            val_true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask\n",
        "                    )\n",
        "\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "                    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "                    val_predictions.extend(predictions.cpu().numpy())\n",
        "                    val_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                best_model_state = self.model.state_dict().copy()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}:')\n",
        "            print(f'Average training loss: {avg_train_loss:.4f}')\n",
        "            print(f'Average validation loss: {avg_val_loss:.4f}')\n",
        "            print(f'GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        if best_model_state is not None:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "\n",
        "        return val_predictions, val_true_labels\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('MIMIC-SBDH.csv')\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# SBDH categories\n",
        "sbdh_columns = [\n",
        "    'sdoh_community_present', 'sdoh_community_absent',\n",
        "    'sdoh_education', 'sdoh_economics', 'sdoh_environment',\n",
        "    'behavior_alcohol', 'behavior_tobacco', 'behavior_drug'\n",
        "]\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "all_fold_results = {}\n",
        "training_params = {}\n",
        "\n",
        "for sbdh in sbdh_columns:\n",
        "    print(f\"\\nProcessing {sbdh}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    X = df['Social_History'].values\n",
        "    y = df[sbdh].values\n",
        "\n",
        "    # Print class distribution\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    class_dist = dict(zip(unique, counts))\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(class_dist)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_scores = []\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\nFold {fold}/5\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        classifier = BioClinicalBERTClassifier(len(np.unique(y)))\n",
        "\n",
        "        train_dataset = SBDHDataset(X_train, y_train, classifier.tokenizer)\n",
        "        val_dataset = SBDHDataset(X_val, y_val, classifier.tokenizer)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "        val_predictions, val_true_labels = classifier.train_model(\n",
        "            train_loader,\n",
        "            val_loader\n",
        "        )\n",
        "\n",
        "        fold_score = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "        fold_scores.append(fold_score)\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold,\n",
        "            'f1_score': fold_score,\n",
        "            'predictions': val_predictions,\n",
        "            'true_labels': val_true_labels\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold} Macro-F1: {fold_score:.4f}\")\n",
        "\n",
        "        del classifier\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    mean_score = np.mean(fold_scores)\n",
        "    std_score = np.std(fold_scores)\n",
        "\n",
        "    results[sbdh] = (mean_score, std_score)\n",
        "    all_fold_results[sbdh] = fold_results\n",
        "\n",
        "    training_params[sbdh] = {\n",
        "        'max_length': 128,\n",
        "        'batch_size': 8,\n",
        "        'learning_rate': 2e-5,\n",
        "        'epochs': 5,\n",
        "        'gradient_accumulation_steps': 4,\n",
        "        'model': 'Bio-ClinicalBERT',\n",
        "        'optimizer': 'AdamW',\n",
        "        'scheduler': 'linear_warmup',\n",
        "        'warmup_steps': 0,\n",
        "        'class_distribution': class_dist\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{sbdh} Final Score: {mean_score:.4f} ± {std_score:.4f}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'SBDH': sbdh,\n",
        "        'Macro-F1 Mean': mean,\n",
        "        'Macro-F1 Std': std,\n",
        "        'Paper Score': {\n",
        "            'sdoh_community_present': '0.9578 ± 0.0027',\n",
        "            'sdoh_community_absent': '0.9068 ± 0.0206',\n",
        "            'sdoh_education': '0.8386 ± 0.0424',\n",
        "            'sdoh_economics': '0.8964 ± 0.0073',\n",
        "            'sdoh_environment': '0.7969 ± 0.0703',\n",
        "            'behavior_alcohol': '0.7049 ± 0.0220',\n",
        "            'behavior_tobacco': '0.7450 ± 0.0222',\n",
        "            'behavior_drug': '0.5812 ± 0.0501'\n",
        "        }[sbdh],\n",
        "        'Best Parameters': training_params[sbdh],\n",
        "        'Class Distribution': str(training_params[sbdh]['class_distribution'])\n",
        "    }\n",
        "    for sbdh, (mean, std) in results.items()\n",
        "])\n",
        "\n",
        "# Create fold results DataFrame\n",
        "fold_df = pd.DataFrame([\n",
        "    {\n",
        "        'SBDH': sbdh,\n",
        "        'Fold': fold_result['fold'],\n",
        "        'F1 Score': fold_result['f1_score']\n",
        "    }\n",
        "    for sbdh, fold_results in all_fold_results.items()\n",
        "    for fold_result in fold_results\n",
        "])\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSummary Results:\")\n",
        "print(\"=\" * 100)\n",
        "display(results_df[['SBDH', 'Macro-F1 Mean', 'Macro-F1 Std', 'Paper Score']])\n",
        "\n",
        "print(\"\\nDetailed Parameters:\")\n",
        "print(\"=\" * 100)\n",
        "params_df = pd.DataFrame(training_params).T\n",
        "display(params_df)\n",
        "\n",
        "print(\"\\nFold Results:\")\n",
        "print(\"=\" * 100)\n",
        "pivot_fold_df = fold_df.pivot(index='SBDH', columns='Fold', values='F1 Score')\n",
        "display(pivot_fold_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PjPOtynXxZN",
        "outputId": "6fce800c-4edc-49c5-c355-c76063c0f66e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\n",
            "Training Bio-ClinicalBERT for sdoh_environment\n",
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized with 3 labels\n",
            "Epoch 1/3:\n",
            "Average training loss: 0.6123\n",
            "Average validation loss: 0.5124\n",
            "Epoch 2/3:\n",
            "Average training loss: 0.4289\n",
            "Average validation loss: 0.4687\n",
            "Epoch 3/3:\n",
            "Average training loss: 0.3789\n",
            "Average validation loss: 0.3960\n",
            "Macro-F1: 0.6307\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SBDHDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):  # Reduced from 256 to 128\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        # Truncate text to reduce processing time\n",
        "        text = ' '.join(text.split()[:50])  # Take first 50 words only\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class BioClinicalBERTClassifier:\n",
        "    def __init__(self, num_labels):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load pre-trained model and tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            'emilyalsentzer/Bio_ClinicalBERT',\n",
        "            num_labels=num_labels,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        print(f\"Model initialized with {num_labels} labels\")\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=3):  # Reduced from 50 to 3\n",
        "        # Calculate class weights\n",
        "        labels = torch.tensor([item['labels'] for item in train_loader.dataset])\n",
        "        class_counts = torch.bincount(labels)\n",
        "        total_samples = len(labels)\n",
        "        class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "        class_weights = class_weights.to(self.device)\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        optimizer = AdamW(self.model.parameters(), lr=2e-5)  # Slightly reduced learning rate\n",
        "\n",
        "        # Gradient Accumulation steps\n",
        "        gradient_accumulation_steps = 4\n",
        "\n",
        "        total_steps = len(train_loader) * epochs // gradient_accumulation_steps\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "                loss = criterion(outputs.logits, labels) / gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Gradient Accumulation\n",
        "                if (i + 1) % gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            avg_train_loss = total_train_loss * gradient_accumulation_steps / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            total_val_loss = 0\n",
        "            val_predictions = []\n",
        "            val_true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask\n",
        "                    )\n",
        "\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "                    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "                    val_predictions.extend(predictions.cpu().numpy())\n",
        "                    val_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}:')\n",
        "            print(f'Average training loss: {avg_train_loss:.4f}')\n",
        "            print(f'Average validation loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        return val_predictions, val_true_labels\n",
        "\n",
        "    def predict(self, texts):\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "\n",
        "        dataset = SBDHDataset(texts, [0]*len(texts), self.tokenizer)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "                batch_predictions = torch.argmax(outputs.logits, dim=1)\n",
        "                predictions.extend(batch_predictions.cpu().numpy())\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load and prepare data\n",
        "print(\"Loading data...\")\n",
        "social_history_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/filtered_social_history_data.csv')\n",
        "sbdh_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MIMIC-SBDH.csv')\n",
        "df = pd.merge(social_history_df, sbdh_df, on='row_id')\n",
        "\n",
        "# Test on a single SBDH first\n",
        "sbdh = 'sdoh_environment'\n",
        "print(f\"\\nTraining Bio-ClinicalBERT for {sbdh}\")\n",
        "\n",
        "X = df['Social_History'].values\n",
        "y = df[sbdh].values\n",
        "\n",
        "# Initialize cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_scores = []\n",
        "\n",
        "# Only run first fold for testing\n",
        "train_idx, val_idx = next(skf.split(X, y))\n",
        "X_train, X_val = X[train_idx], X[val_idx]\n",
        "y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "# Initialize classifier\n",
        "num_labels = len(np.unique(y))\n",
        "classifier = BioClinicalBERTClassifier(num_labels)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SBDHDataset(X_train, y_train, classifier.tokenizer)\n",
        "val_dataset = SBDHDataset(X_val, y_val, classifier.tokenizer)\n",
        "\n",
        "# Create dataloaders with smaller batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Train and evaluate\n",
        "val_predictions, val_true_labels = classifier.train_model(\n",
        "    train_loader,\n",
        "    val_loader\n",
        ")\n",
        "\n",
        "# Calculate score\n",
        "fold_score = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "print(f\"Macro-F1: {fold_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H84766hJL1nh"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}